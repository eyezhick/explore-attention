{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Attention in Audio Processing\n",
       "\n",
       "This notebook explores how attention mechanisms are applied in audio processing tasks. We'll cover:\n",
       "\n",
       "1. Audio feature extraction\n",
       "2. Attention in speech recognition\n",
       "3. Attention in audio classification\n",
       "4. Visualizing audio attention patterns"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Why Attention in Audio?\n",
       "\n",
       "Audio processing presents unique challenges:\n",
       "\n",
       "1. **Long Sequences**: Audio signals can be very long\n",
       "2. **Temporal Dependencies**: Important information can be spread across time\n",
       "3. **Variable Length**: Different audio samples have different durations\n",
       "\n",
       "Attention helps by:\n",
       "\n",
       "- Focusing on relevant parts of the audio signal\n",
       "- Handling variable-length sequences\n",
       "- Capturing long-range dependencies"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torchaudio\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from typing import Tuple, Optional\n",
       "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Audio Feature Extraction\n",
       "\n",
       "First, let's implement a simple audio feature extractor:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class AudioFeatureExtractor(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        n_mels: int = 80,\n",
       "        n_fft: int = 400,\n",
       "        hop_length: int = 160,\n",
       "        win_length: int = 400\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Mel spectrogram parameters\n",
       "        self.n_mels = n_mels\n",
       "        self.n_fft = n_fft\n",
       "        self.hop_length = hop_length\n",
       "        self.win_length = win_length\n",
       "        \n",
       "        # Mel spectrogram transform\n",
       "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
       "            sample_rate=16000,\n",
       "            n_fft=n_fft,\n",
       "            hop_length=hop_length,\n",
       "            win_length=win_length,\n",
       "            n_mels=n_mels\n",
       "        )\n",
       "        \n",
       "        # Log-mel transform\n",
       "        self.log_transform = torchaudio.transforms.AmplitudeToDB()\n",
       "        \n",
       "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
       "        \"\"\"Extract mel spectrogram features from waveform.\"\"\"\n",
       "        # Compute mel spectrogram\n",
       "        mel_spec = self.mel_transform(waveform)\n",
       "        \n",
       "        # Convert to log scale\n",
       "        log_mel_spec = self.log_transform(mel_spec)\n",
       "        \n",
       "        return log_mel_spec"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Audio Attention Model\n",
       "\n",
       "Now, let's implement an attention-based audio processing model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class AudioAttentionModel(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        input_dim: int,\n",
       "        hidden_dim: int,\n",
       "        num_heads: int,\n",
       "        num_classes: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Feature projection\n",
       "        self.projection = nn.Linear(input_dim, hidden_dim)\n",
       "        \n",
       "        # Multi-head attention\n",
       "        self.attention = nn.MultiheadAttention(\n",
       "            embed_dim=hidden_dim,\n",
       "            num_heads=num_heads,\n",
       "            dropout=dropout\n",
       "        )\n",
       "        \n",
       "        # Layer normalization\n",
       "        self.norm = nn.LayerNorm(hidden_dim)\n",
       "        \n",
       "        # Classification head\n",
       "        self.classifier = nn.Sequential(\n",
       "            nn.Linear(hidden_dim, hidden_dim),\n",
       "            nn.ReLU(),\n",
       "            nn.Dropout(dropout),\n",
       "            nn.Linear(hidden_dim, num_classes)\n",
       "        )\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        x: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        # Project features\n",
       "        x = self.projection(x)\n",
       "        \n",
       "        # Transpose for attention (sequence first)\n",
       "        x = x.transpose(0, 1)\n",
       "        \n",
       "        # Apply attention\n",
       "        attn_output, attn_weights = self.attention(x, x, x, key_padding_mask=mask)\n",
       "        \n",
       "        # Layer normalization\n",
       "        x = self.norm(attn_output)\n",
       "        \n",
       "        # Transpose back (batch first)\n",
       "        x = x.transpose(0, 1)\n",
       "        \n",
       "        # Global average pooling\n",
       "        x = x.mean(dim=1)\n",
       "        \n",
       "        # Classification\n",
       "        logits = self.classifier(x)\n",
       "        \n",
       "        return logits, attn_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visualizing Audio Attention\n",
       "\n",
       "Let's create functions to visualize attention patterns in audio:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def plot_audio_attention(\n",
       "    waveform: torch.Tensor,\n",
       "    attention_weights: torch.Tensor,\n",
       "    sample_rate: int = 16000,\n",
       "    title: str = \"Audio Attention Pattern\"\n",
       ") -> None:\n",
       "    \"\"\"Plot audio waveform with attention weights.\"\"\"\n",
       "    # Create figure\n",
       "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
       "    \n",
       "    # Plot waveform\n",
       "    time = np.arange(len(waveform)) / sample_rate\n",
       "    ax1.plot(time, waveform.numpy())\n",
       "    ax1.set_title(\"Audio Waveform\")\n",
       "    ax1.set_xlabel(\"Time (s)\")\n",
       "    ax1.set_ylabel(\"Amplitude\")\n",
       "    \n",
       "    # Plot attention weights\n",
       "    sns.heatmap(\n",
       "        attention_weights,\n",
       "        ax=ax2,\n",
       "        cmap='viridis',\n",
       "        xticklabels=100,\n",
       "        yticklabels=100\n",
       "    )\n",
       "    ax2.set_title(\"Attention Weights\")\n",
       "    ax2.set_xlabel(\"Time Step\")\n",
       "    ax2.set_ylabel(\"Time Step\")\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "def plot_mel_attention(\n",
       "    mel_spec: torch.Tensor,\n",
       "    attention_weights: torch.Tensor,\n",
       "    title: str = \"Mel Spectrogram with Attention\"\n",
       ") -> None:\n",
       "    \"\"\"Plot mel spectrogram with attention weights.\"\"\"\n",
       "    # Create figure\n",
       "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
       "    \n",
       "    # Plot mel spectrogram\n",
       "    im1 = ax1.imshow(\n",
       "        mel_spec.numpy(),\n",
       "        aspect='auto',\n",
       "        origin='lower',\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    ax1.set_title(\"Mel Spectrogram\")\n",
       "    ax1.set_xlabel(\"Time Step\")\n",
       "    ax1.set_ylabel(\"Mel Bin\")\n",
       "    plt.colorbar(im1, ax=ax1)\n",
       "    \n",
       "    # Plot attention weights\n",
       "    im2 = ax2.imshow(\n",
       "        attention_weights.numpy(),\n",
       "        aspect='auto',\n",
       "        origin='lower',\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    ax2.set_title(\"Attention Weights\")\n",
       "    ax2.set_xlabel(\"Time Step\")\n",
       "    ax2.set_ylabel(\"Time Step\")\n",
       "    plt.colorbar(im2, ax=ax2)\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Real-World Example: Speech Recognition\n",
       "\n",
       "Let's analyze attention patterns in a pre-trained speech recognition model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load pre-trained model and processor\n",
       "model_name = 'facebook/wav2vec2-base-960h'\n",
       "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
       "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
       "\n",
       "def analyze_speech_attention(audio_path: str) -> None:\n",
       "    \"\"\"Analyze attention patterns in speech recognition.\"\"\"\n",
       "    # Load audio\n",
       "    waveform, sample_rate = torchaudio.load(audio_path)\n",
       "    \n",
       "    # Resample if necessary\n",
       "    if sample_rate != 16000:\n",
       "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
       "        waveform = resampler(waveform)\n",
       "    \n",
       "    # Process audio\n",
       "    inputs = processor(\n",
       "        waveform.squeeze().numpy(),\n",
       "        sampling_rate=16000,\n",
       "        return_tensors=\"pt\"\n",
       "    )\n",
       "    \n",
       "    # Get model output with attention\n",
       "    with torch.no_grad():\n",
       "        outputs = model(**inputs, output_attentions=True)\n",
       "    \n",
       "    # Get attention weights from the last layer\n",
       "    attention_weights = outputs.attentions[-1][0, 0]  # First batch, first head\n",
       "    \n",
       "    # Plot attention patterns\n",
       "    plot_audio_attention(\n",
       "        waveform.squeeze(),\n",
       "        attention_weights,\n",
       "        title=\"Speech Recognition Attention Pattern\"\n",
       "    )\n",
       "    \n",
       "    # Decode transcription\n",
       "    predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
       "    transcription = processor.batch_decode(predicted_ids)\n",
       "    print(f\"Transcription: {transcription[0]}\")\n",
       "\n",
       "# Example usage (replace with your audio file)\n",
       "# analyze_speech_attention('path_to_audio.wav')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Audio Classification Example\n",
       "\n",
       "Let's implement a simple audio classification task using attention:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def train_audio_classifier(\n",
       "    model: AudioAttentionModel,\n",
       "    train_loader: torch.utils.data.DataLoader,\n",
       "    num_epochs: int,\n",
       "    learning_rate: float = 0.001\n",
       ") -> List[float]:\n",
       "    \"\"\"Train the audio classification model.\"\"\"\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
       "    criterion = nn.CrossEntropyLoss()\n",
       "    \n",
       "    losses = []\n",
       "    \n",
       "    for epoch in range(num_epochs):\n",
       "        epoch_loss = 0\n",
       "        \n",
       "        for batch in train_loader:\n",
       "            # Get batch data\n",
       "            waveforms, labels = batch\n",
       "            \n",
       "            # Forward pass\n",
       "            logits, _ = model(waveforms)\n",
       "            \n",
       "            # Compute loss\n",
       "            loss = criterion(logits, labels)\n",
       "            \n",
       "            # Backward pass\n",
       "            optimizer.zero_grad()\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "            \n",
       "            epoch_loss += loss.item()\n",
       "        \n",
       "        # Record average loss\n",
       "        avg_loss = epoch_loss / len(train_loader)\n",
       "        losses.append(avg_loss)\n",
       "        \n",
       "        if (epoch + 1) % 10 == 0:\n",
       "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
       "    \n",
       "    return losses"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. Audio feature extraction\n",
       "2. Attention-based audio processing\n",
       "3. Visualization of audio attention patterns\n",
       "4. Real-world applications in speech recognition\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Attention is particularly useful for audio due to its sequential nature\n",
       "- Different attention patterns can capture different aspects of audio\n",
       "- Visualization helps understand how the model processes audio\n",
       "\n",
       "In the next notebook, we'll explore attention in multimodal systems that combine audio with other modalities."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 