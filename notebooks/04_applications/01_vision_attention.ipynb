{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Attention in Computer Vision\n",
       "\n",
       "This notebook explores how attention mechanisms are applied in computer vision tasks. We'll cover:\n",
       "\n",
       "1. Vision Transformers (ViT)\n",
       "2. Attention in image classification\n",
       "3. Visualizing attention maps\n",
       "4. Real-world applications"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Why Attention in Vision?\n",
       "\n",
       "Traditional CNNs have limitations:\n",
       "\n",
       "1. **Local Receptive Fields**: CNNs process images through local convolutions\n",
       "2. **Fixed Architecture**: The architecture is predetermined by the network design\n",
       "3. **Limited Global Context**: Capturing long-range dependencies requires deep networks\n",
       "\n",
       "Attention in vision addresses these by:\n",
       "\n",
       "- Enabling direct modeling of relationships between any image regions\n",
       "- Providing interpretable attention maps\n",
       "- Allowing flexible architecture design"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torchvision\n",
       "import torchvision.transforms as transforms\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from PIL import Image\n",
       "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Vision Transformer (ViT) Overview\n",
       "\n",
       "The Vision Transformer (ViT) applies the Transformer architecture to images by:\n",
       "\n",
       "1. **Image Patching**: Dividing the image into fixed-size patches\n",
       "2. **Linear Projection**: Flattening and projecting patches into tokens\n",
       "3. **Position Embeddings**: Adding learnable position embeddings\n",
       "4. **Transformer Encoder**: Processing tokens through self-attention layers"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def load_and_preprocess_image(image_path):\n",
       "    \"\"\"Load and preprocess an image for ViT.\"\"\"\n",
       "    # Load image\n",
       "    image = Image.open(image_path).convert('RGB')\n",
       "    \n",
       "    # Define preprocessing\n",
       "    transform = transforms.Compose([\n",
       "        transforms.Resize((224, 224)),\n",
       "        transforms.ToTensor(),\n",
       "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
       "                           std=[0.229, 0.224, 0.225])\n",
       "    ])\n",
       "    \n",
       "    # Apply preprocessing\n",
       "    image_tensor = transform(image).unsqueeze(0)\n",
       "    \n",
       "    return image, image_tensor\n",
       "\n",
       "def visualize_attention_map(image, attention_weights, patch_size=16):\n",
       "    \"\"\"Visualize attention weights as a heatmap over the image.\"\"\"\n",
       "    # Reshape attention weights to match image patches\n",
       "    h, w = image.size[1] // patch_size, image.size[0] // patch_size\n",
       "    attention_map = attention_weights.reshape(h, w)\n",
       "    \n",
       "    # Create figure\n",
       "    plt.figure(figsize=(12, 4))\n",
       "    \n",
       "    # Plot original image\n",
       "    plt.subplot(1, 2, 1)\n",
       "    plt.imshow(image)\n",
       "    plt.title('Original Image')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    # Plot attention map\n",
       "    plt.subplot(1, 2, 2)\n",
       "    plt.imshow(image)\n",
       "    plt.imshow(attention_map, alpha=0.5, cmap='jet')\n",
       "    plt.title('Attention Map')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Loading a Pre-trained ViT Model\n",
       "\n",
       "Let's load a pre-trained ViT model and examine its attention patterns:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load pre-trained ViT model\n",
       "model_name = 'google/vit-base-patch16-224'\n",
       "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
       "model = ViTForImageClassification.from_pretrained(model_name)\n",
       "\n",
       "# Set model to evaluation mode\n",
       "model.eval()\n",
       "\n",
       "def get_attention_maps(model, image_tensor):\n",
       "    \"\"\"Extract attention maps from the model.\"\"\"\n",
       "    with torch.no_grad():\n",
       "        outputs = model(image_tensor, output_attentions=True)\n",
       "        \n",
       "    # Get attention weights from the last layer\n",
       "    attention_weights = outputs.attentions[-1][0, 0]  # First batch, first head\n",
       "    \n",
       "    # Average attention weights for the [CLS] token\n",
       "    cls_attention = attention_weights[0, 1:].mean(dim=0)\n",
       "    \n",
       "    return cls_attention"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visualizing Attention in Image Classification\n",
       "\n",
       "Let's analyze how the model attends to different parts of an image:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load and process an example image\n",
       "image_path = 'path_to_your_image.jpg'  # Replace with your image path\n",
       "image, image_tensor = load_and_preprocess_image(image_path)\n",
       "\n",
       "# Get attention maps\n",
       "attention_weights = get_attention_maps(model, image_tensor)\n",
       "\n",
       "# Visualize attention\n",
       "visualize_attention_map(image, attention_weights)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Analyzing Different Attention Heads\n",
       "\n",
       "Different attention heads in ViT can focus on different aspects of the image:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def visualize_multi_head_attention(model, image_tensor, num_heads=4):\n",
       "    \"\"\"Visualize attention patterns from multiple heads.\"\"\"\n",
       "    with torch.no_grad():\n",
       "        outputs = model(image_tensor, output_attentions=True)\n",
       "    \n",
       "    # Get attention weights from the last layer\n",
       "    attention_weights = outputs.attentions[-1][0]  # First batch\n",
       "    \n",
       "    # Create figure for visualization\n",
       "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
       "    axes = axes.flatten()\n",
       "    \n",
       "    # Visualize attention for selected heads\n",
       "    for i in range(num_heads):\n",
       "        # Get attention weights for the [CLS] token\n",
       "        cls_attention = attention_weights[i, 0, 1:].mean(dim=0)\n",
       "        \n",
       "        # Reshape to match image patches\n",
       "        h, w = 14, 14  # For 224x224 images with 16x16 patches\n",
       "        attention_map = cls_attention.reshape(h, w)\n",
       "        \n",
       "        # Plot attention map\n",
       "        axes[i].imshow(image)\n",
       "        axes[i].imshow(attention_map, alpha=0.5, cmap='jet')\n",
       "        axes[i].set_title(f'Head {i+1}')\n",
       "        axes[i].axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "# Visualize multiple attention heads\n",
       "visualize_multi_head_attention(model, image_tensor)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Real-World Applications\n",
       "\n",
       "Attention mechanisms in vision have enabled several breakthroughs:\n",
       "\n",
       "1. **Image Classification**: ViT achieves state-of-the-art results\n",
       "2. **Object Detection**: DETR uses attention for end-to-end object detection\n",
       "3. **Image Generation**: Attention helps in generating high-quality images\n",
       "4. **Medical Imaging**: Attention helps focus on relevant regions in medical images"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. How attention mechanisms are applied in computer vision\n",
       "2. The Vision Transformer architecture\n",
       "3. Visualizing attention patterns in images\n",
       "4. Real-world applications of attention in vision\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Attention provides a powerful way to model relationships in images\n",
       "- Different attention heads can focus on different aspects of the image\n",
       "- Attention maps provide interpretable insights into model decisions\n",
       "\n",
       "In the next notebook, we'll explore attention in other domains like audio and multimodal systems."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 