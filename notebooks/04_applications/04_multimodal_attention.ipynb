{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Attention in Multimodal Systems\n",
       "\n",
       "This notebook explores how attention mechanisms are applied in multimodal systems that combine different types of data. We'll cover:\n",
       "\n",
       "1. Cross-modal attention\n",
       "2. Multimodal fusion\n",
       "3. Visual question answering\n",
       "4. Audio-visual learning"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Why Attention in Multimodal Learning?\n",
       "\n",
       "Multimodal learning presents unique challenges:\n",
       "\n",
       "1. **Different Modalities**: Each modality has its own characteristics\n",
       "2. **Alignment**: Need to align information across modalities\n",
       "3. **Fusion**: Need to combine information effectively\n",
       "\n",
       "Attention helps by:\n",
       "\n",
       "- Learning cross-modal relationships\n",
       "- Focusing on relevant parts of each modality\n",
       "- Enabling flexible fusion strategies"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torchvision\n",
       "import torchaudio\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from typing import Tuple, Optional, List\n",
       "from transformers import (\n",
       "    ViTFeatureExtractor,\n",
       "    ViTModel,\n",
       "    Wav2Vec2Processor,\n",
       "    Wav2Vec2Model\n",
       ")\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Cross-Modal Attention\n",
       "\n",
       "Let's implement a cross-modal attention mechanism:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class CrossModalAttention(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        query_dim: int,\n",
       "        key_dim: int,\n",
       "        value_dim: int,\n",
       "        num_heads: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        self.num_heads = num_heads\n",
       "        self.head_dim = query_dim // num_heads\n",
       "        \n",
       "        # Linear projections\n",
       "        self.q_proj = nn.Linear(query_dim, query_dim)\n",
       "        self.k_proj = nn.Linear(key_dim, query_dim)\n",
       "        self.v_proj = nn.Linear(value_dim, query_dim)\n",
       "        self.out_proj = nn.Linear(query_dim, query_dim)\n",
       "        \n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        self.scale = math.sqrt(self.head_dim)\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        query: torch.Tensor,\n",
       "        key: torch.Tensor,\n",
       "        value: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        batch_size = query.size(0)\n",
       "        \n",
       "        # Project Q, K, V\n",
       "        q = self.q_proj(query)\n",
       "        k = self.k_proj(key)\n",
       "        v = self.v_proj(value)\n",
       "        \n",
       "        # Reshape for multi-head attention\n",
       "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        \n",
       "        # Compute attention scores\n",
       "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
       "        \n",
       "        # Apply mask if provided\n",
       "        if mask is not None:\n",
       "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
       "        \n",
       "        # Compute attention weights\n",
       "        attention_weights = F.softmax(scores, dim=-1)\n",
       "        attention_weights = self.dropout(attention_weights)\n",
       "        \n",
       "        # Apply attention weights to values\n",
       "        context = torch.matmul(attention_weights, v)\n",
       "        \n",
       "        # Reshape back\n",
       "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
       "        \n",
       "        # Project output\n",
       "        output = self.out_proj(context)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Multimodal Fusion Model\n",
       "\n",
       "Now, let's implement a multimodal fusion model using attention:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class MultimodalFusionModel(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        visual_dim: int,\n",
       "        audio_dim: int,\n",
       "        text_dim: int,\n",
       "        hidden_dim: int,\n",
       "        num_heads: int,\n",
       "        num_classes: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Modality-specific encoders\n",
       "        self.visual_encoder = nn.Linear(visual_dim, hidden_dim)\n",
       "        self.audio_encoder = nn.Linear(audio_dim, hidden_dim)\n",
       "        self.text_encoder = nn.Linear(text_dim, hidden_dim)\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        self.visual_audio_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        self.audio_visual_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        self.text_visual_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        \n",
       "        # Layer normalization\n",
       "        self.norm = nn.LayerNorm(hidden_dim)\n",
       "        \n",
       "        # Classification head\n",
       "        self.classifier = nn.Sequential(\n",
       "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
       "            nn.ReLU(),\n",
       "            nn.Dropout(dropout),\n",
       "            nn.Linear(hidden_dim, num_classes)\n",
       "        )\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        visual: torch.Tensor,\n",
       "        audio: torch.Tensor,\n",
       "        text: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
       "        # Encode modalities\n",
       "        visual_features = self.visual_encoder(visual)\n",
       "        audio_features = self.audio_encoder(audio)\n",
       "        text_features = self.text_encoder(text)\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        visual_audio, va_weights = self.visual_audio_attention(visual_features, audio_features, audio_features, mask)\n",
       "        audio_visual, av_weights = self.audio_visual_attention(audio_features, visual_features, visual_features, mask)\n",
       "        text_visual, tv_weights = self.text_visual_attention(text_features, visual_features, visual_features, mask)\n",
       "        \n",
       "        # Layer normalization\n",
       "        visual_audio = self.norm(visual_audio)\n",
       "        audio_visual = self.norm(audio_visual)\n",
       "        text_visual = self.norm(text_visual)\n",
       "        \n",
       "        # Global average pooling\n",
       "        visual_audio = visual_audio.mean(dim=1)\n",
       "        audio_visual = audio_visual.mean(dim=1)\n",
       "        text_visual = text_visual.mean(dim=1)\n",
       "        \n",
       "        # Concatenate features\n",
       "        combined = torch.cat([visual_audio, audio_visual, text_visual], dim=1)\n",
       "        \n",
       "        # Classification\n",
       "        logits = self.classifier(combined)\n",
       "        \n",
       "        return logits, [va_weights, av_weights, tv_weights]"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visual Question Answering Example\n",
       "\n",
       "Let's implement a simple visual question answering system:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class VisualQuestionAnswering(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        visual_dim: int,\n",
       "        text_dim: int,\n",
       "        hidden_dim: int,\n",
       "        num_heads: int,\n",
       "        vocab_size: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Visual encoder (ViT)\n",
       "        self.visual_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
       "        \n",
       "        # Text encoder (BERT)\n",
       "        self.text_encoder = nn.Linear(text_dim, hidden_dim)\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        self.visual_question_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        self.question_visual_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        \n",
       "        # Answer prediction\n",
       "        self.answer_predictor = nn.Sequential(\n",
       "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
       "            nn.ReLU(),\n",
       "            nn.Dropout(dropout),\n",
       "            nn.Linear(hidden_dim, vocab_size)\n",
       "        )\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        image: torch.Tensor,\n",
       "        question: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
       "        # Encode image\n",
       "        visual_features = self.visual_encoder(image).last_hidden_state\n",
       "        \n",
       "        # Encode question\n",
       "        question_features = self.text_encoder(question)\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        visual_question, vq_weights = self.visual_question_attention(\n",
       "            visual_features, question_features, question_features, mask\n",
       "        )\n",
       "        question_visual, qv_weights = self.question_visual_attention(\n",
       "            question_features, visual_features, visual_features, mask\n",
       "        )\n",
       "        \n",
       "        # Combine features\n",
       "        combined = torch.cat([visual_question, question_visual], dim=-1)\n",
       "        \n",
       "        # Predict answer\n",
       "        logits = self.answer_predictor(combined)\n",
       "        \n",
       "        return logits, [vq_weights, qv_weights]"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Audio-Visual Learning Example\n",
       "\n",
       "Let's implement an audio-visual learning model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class AudioVisualLearning(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        visual_dim: int,\n",
       "        audio_dim: int,\n",
       "        hidden_dim: int,\n",
       "        num_heads: int,\n",
       "        num_classes: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Visual encoder (ViT)\n",
       "        self.visual_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
       "        \n",
       "        # Audio encoder (Wav2Vec2)\n",
       "        self.audio_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        self.visual_audio_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        self.audio_visual_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim, num_heads)\n",
       "        \n",
       "        # Classification head\n",
       "        self.classifier = nn.Sequential(\n",
       "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
       "            nn.ReLU(),\n",
       "            nn.Dropout(dropout),\n",
       "            nn.Linear(hidden_dim, num_classes)\n",
       "        )\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        image: torch.Tensor,\n",
       "        audio: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
       "        # Encode image\n",
       "        visual_features = self.visual_encoder(image).last_hidden_state\n",
       "        \n",
       "        # Encode audio\n",
       "        audio_features = self.audio_encoder(audio).last_hidden_state\n",
       "        \n",
       "        # Cross-modal attention\n",
       "        visual_audio, va_weights = self.visual_audio_attention(\n",
       "            visual_features, audio_features, audio_features, mask\n",
       "        )\n",
       "        audio_visual, av_weights = self.audio_visual_attention(\n",
       "            audio_features, visual_features, visual_features, mask\n",
       "        )\n",
       "        \n",
       "        # Combine features\n",
       "        combined = torch.cat([visual_audio, audio_visual], dim=-1)\n",
       "        \n",
       "        # Classification\n",
       "        logits = self.classifier(combined)\n",
       "        \n",
       "        return logits, [va_weights, av_weights]"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visualizing Cross-Modal Attention\n",
       "\n",
       "Let's create functions to visualize cross-modal attention patterns:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def plot_cross_modal_attention(\n",
       "    attention_weights: torch.Tensor,\n",
       "    source_labels: List[str],\n",
       "    target_labels: List[str],\n",
       "    title: str = \"Cross-Modal Attention\"\n",
       ") -> None:\n",
       "    \"\"\"Plot cross-modal attention weights.\"\"\"\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    sns.heatmap(\n",
       "        attention_weights,\n",
       "        xticklabels=target_labels,\n",
       "        yticklabels=source_labels,\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    plt.title(title)\n",
       "    plt.xlabel('Target Modality')\n",
       "    plt.ylabel('Source Modality')\n",
       "    plt.show()\n",
       "\n",
       "def visualize_multimodal_attention(\n",
       "    model: nn.Module,\n",
       "    image: torch.Tensor,\n",
       "    audio: torch.Tensor,\n",
       "    text: Optional[torch.Tensor] = None\n",
       ") -> None:\n",
       "    \"\"\"Visualize attention patterns in a multimodal model.\"\"\"\n",
       "    # Get model output\n",
       "    with torch.no_grad():\n",
       "        if text is not None:\n",
       "            logits, attention_weights = model(image, audio, text)\n",
       "        else:\n",
       "            logits, attention_weights = model(image, audio)\n",
       "    \n",
       "    # Plot attention patterns\n",
       "    for i, weights in enumerate(attention_weights):\n",
       "        plot_cross_modal_attention(\n",
       "            weights[0, 0],  # First batch, first head\n",
       "            source_labels=[f\"Source {j}\" for j in range(weights.size(2))],\n",
       "            target_labels=[f\"Target {j}\" for j in range(weights.size(3))],\n",
       "            title=f\"Attention Pattern {i+1}\"\n",
       "        )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. Cross-modal attention mechanisms\n",
       "2. Multimodal fusion using attention\n",
       "3. Visual question answering\n",
       "4. Audio-visual learning\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Attention provides a powerful way to model relationships between modalities\n",
       "- Cross-modal attention helps align information across different types of data\n",
       "- Visualization helps understand how the model processes multimodal information\n",
       "\n",
       "This concludes our exploration of attention mechanisms in various domains. The next notebook will focus on advanced topics and future directions."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }