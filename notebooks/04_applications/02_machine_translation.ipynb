{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Attention in Machine Translation\n",
       "\n",
       "This notebook explores how attention mechanisms revolutionized machine translation. We'll cover:\n",
       "\n",
       "1. The evolution of attention in translation\n",
       "2. Implementation of attention-based translation\n",
       "3. Visualizing attention patterns\n",
       "4. Real-world examples and analysis"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## The Evolution of Attention in Translation\n",
       "\n",
       "Machine translation has evolved through several stages:\n",
       "\n",
       "1. **Statistical Machine Translation (SMT)**:\n",
       "   - Rule-based systems\n",
       "   - Phrase-based translation\n",
       "   - Limited context understanding\n",
       "\n",
       "2. **Neural Machine Translation (NMT) with RNNs**:\n",
       "   - Encoder-decoder architecture\n",
       "   - Fixed-size context vector\n",
       "   - Limited long-range dependencies\n",
       "\n",
       "3. **Attention-based NMT**:\n",
       "   - Dynamic context vector\n",
       "   - Direct access to source words\n",
       "   - Better handling of long sequences\n",
       "\n",
       "4. **Transformer-based NMT**:\n",
       "   - Self-attention mechanism\n",
       "   - Parallel processing\n",
       "   - State-of-the-art performance"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from transformers import MarianMTModel, MarianTokenizer\n",
       "from typing import List, Tuple\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Implementing Attention-based Translation\n",
       "\n",
       "Let's implement a simple attention-based translation model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class AttentionTranslation(nn.Module):\n",
       "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
       "        super().__init__()\n",
       "        self.encoder = nn.LSTM(input_dim, hidden_dim, bidirectional=True)\n",
       "        self.decoder = nn.LSTM(hidden_dim * 2, hidden_dim)\n",
       "        self.attention = nn.Linear(hidden_dim * 3, 1)\n",
       "        self.output = nn.Linear(hidden_dim, output_dim)\n",
       "        \n",
       "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        # Encode source sequence\n",
       "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
       "        \n",
       "        # Initialize decoder hidden state\n",
       "        decoder_hidden = hidden[-1].unsqueeze(0)\n",
       "        decoder_cell = cell[-1].unsqueeze(0)\n",
       "        \n",
       "        # Initialize attention weights storage\n",
       "        attention_weights = []\n",
       "        outputs = []\n",
       "        \n",
       "        # Decode target sequence\n",
       "        for t in range(tgt.size(1)):\n",
       "            # Compute attention scores\n",
       "            attention_input = torch.cat([\n",
       "                decoder_hidden.repeat(encoder_outputs.size(1), 1, 1).transpose(0, 1),\n",
       "                encoder_outputs\n",
       "            ], dim=2)\n",
       "            attention_scores = self.attention(attention_input).squeeze(2)\n",
       "            attention_weights_t = F.softmax(attention_scores, dim=1)\n",
       "            attention_weights.append(attention_weights_t)\n",
       "            \n",
       "            # Compute context vector\n",
       "            context = torch.bmm(attention_weights_t.unsqueeze(1), encoder_outputs)\n",
       "            \n",
       "            # Decode one step\n",
       "            decoder_input = torch.cat([tgt[:, t:t+1], context], dim=2)\n",
       "            output, (decoder_hidden, decoder_cell) = self.decoder(decoder_input, (decoder_hidden, decoder_cell))\n",
       "            \n",
       "            # Project to output vocabulary\n",
       "            output = self.output(output)\n",
       "            outputs.append(output)\n",
       "        \n",
       "        return torch.cat(outputs, dim=1), torch.stack(attention_weights, dim=1)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visualizing Attention Patterns\n",
       "\n",
       "Let's create functions to visualize attention patterns in translation:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def plot_attention_alignment(\n",
       "    attention_weights: torch.Tensor,\n",
       "    source_tokens: List[str],\n",
       "    target_tokens: List[str],\n",
       "    title: str = \"Attention Alignment\"\n",
       ") -> None:\n",
       "    \"\"\"Plot attention alignment between source and target tokens.\"\"\"\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    sns.heatmap(\n",
       "        attention_weights,\n",
       "        xticklabels=source_tokens,\n",
       "        yticklabels=target_tokens,\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    plt.title(title)\n",
       "    plt.xlabel('Source Tokens')\n",
       "    plt.ylabel('Target Tokens')\n",
       "    plt.show()\n",
       "\n",
       "def analyze_translation_attention(\n",
       "    model: MarianMTModel,\n",
       "    tokenizer: MarianTokenizer,\n",
       "    text: str,\n",
       "    layer_idx: int = -1\n",
       ") -> None:\n",
       "    \"\"\"Analyze attention patterns in a translation model.\"\"\"\n",
       "    # Tokenize input\n",
       "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
       "    \n",
       "    # Generate translation with attention\n",
       "    outputs = model.generate(**inputs, output_attentions=True)\n",
       "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "    \n",
       "    print(f\"Source: {text}\")\n",
       "    print(f\"Target: {translation}\")\n",
       "    \n",
       "    # Get attention weights\n",
       "    attention_weights = outputs.attentions[layer_idx][0, 0]  # First batch, first head\n",
       "    source_tokens = tokenizer.tokenize(text)\n",
       "    target_tokens = tokenizer.tokenize(translation)\n",
       "    \n",
       "    # Plot attention alignment\n",
       "    plot_attention_alignment(\n",
       "        attention_weights,\n",
       "        source_tokens,\n",
       "        target_tokens,\n",
       "        title=f\"Attention Alignment (Layer {layer_idx})\"\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Real-World Example: MarianMT\n",
       "\n",
       "Let's analyze attention patterns in a pre-trained MarianMT model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load pre-trained model and tokenizer\n",
       "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
       "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
       "model = MarianMTModel.from_pretrained(model_name)\n",
       "\n",
       "# Example translations\n",
       "examples = [\n",
       "    \"The cat sat on the mat.\",\n",
       "    \"Attention mechanisms have revolutionized machine translation.\",\n",
       "    \"Neural networks can learn complex patterns from data.\"\n",
       "]\n",
       "\n",
       "# Analyze attention for each example\n",
       "for text in examples:\n",
       "    analyze_translation_attention(model, tokenizer, text)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Analyzing Different Layers\n",
       "\n",
       "Let's examine how attention patterns vary across different layers:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def analyze_layer_attention(\n",
       "    model: MarianMTModel,\n",
       "    tokenizer: MarianTokenizer,\n",
       "    text: str,\n",
       "    num_layers: int = 3\n",
       ") -> None:\n",
       "    \"\"\"Analyze attention patterns across different layers.\"\"\"\n",
       "    # Tokenize input\n",
       "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
       "    \n",
       "    # Generate translation with attention\n",
       "    outputs = model.generate(**inputs, output_attentions=True)\n",
       "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "    \n",
       "    print(f\"Source: {text}\")\n",
       "    print(f\"Target: {translation}\")\n",
       "    \n",
       "    # Get attention weights for different layers\n",
       "    source_tokens = tokenizer.tokenize(text)\n",
       "    target_tokens = tokenizer.tokenize(translation)\n",
       "    \n",
       "    # Plot attention for each layer\n",
       "    for layer_idx in range(-num_layers, 0):\n",
       "        attention_weights = outputs.attentions[layer_idx][0, 0]\n",
       "        plot_attention_alignment(\n",
       "            attention_weights,\n",
       "            source_tokens,\n",
       "            target_tokens,\n",
       "            title=f\"Attention Alignment (Layer {layer_idx})\"\n",
       "        )\n",
       "\n",
       "# Analyze layer attention for an example\n",
       "text = \"The cat sat on the mat and watched the bird.\"\n",
       "analyze_layer_attention(model, tokenizer, text)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. The evolution of attention in machine translation\n",
       "2. Implementation of an attention-based translation model\n",
       "3. Visualization of attention patterns\n",
       "4. Analysis of real-world translation models\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Attention mechanisms have revolutionized machine translation\n",
       "- Different layers capture different aspects of the translation process\n",
       "- Attention patterns provide insights into how the model makes decisions\n",
       "\n",
       "In the next notebook, we'll explore attention in other domains like audio and multimodal systems."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 