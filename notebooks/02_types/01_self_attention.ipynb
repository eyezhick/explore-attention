{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Self-Attention Mechanism\n",
       "\n",
       "This notebook explores self-attention, a fundamental component of Transformer models. We'll cover:\n",
       "\n",
       "1. The intuition behind self-attention\n",
       "2. Implementation of self-attention\n",
       "3. Visualizing self-attention patterns\n",
       "4. Real-world examples"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from typing import Optional, Tuple\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Self-Attention Implementation\n",
       "\n",
       "Let's implement a self-attention mechanism:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class SelfAttention(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        input_dim: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        self.input_dim = input_dim\n",
       "        \n",
       "        # Linear projections for Q, K, V\n",
       "        self.query = nn.Linear(input_dim, input_dim)\n",
       "        self.key = nn.Linear(input_dim, input_dim)\n",
       "        self.value = nn.Linear(input_dim, input_dim)\n",
       "        \n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        self.scale = torch.sqrt(torch.FloatTensor([input_dim]))\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        x: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        \"\"\"\n",
       "        Args:\n",
       "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
       "            mask: Optional mask tensor of shape (batch_size, seq_len, seq_len)\n",
       "            \n",
       "        Returns:\n",
       "            Tuple of (output, attention_weights)\n",
       "        \"\"\"\n",
       "        batch_size = x.shape[0]\n",
       "        \n",
       "        # Project input to Q, K, V\n",
       "        Q = self.query(x)  # (batch_size, seq_len, input_dim)\n",
       "        K = self.key(x)    # (batch_size, seq_len, input_dim)\n",
       "        V = self.value(x)  # (batch_size, seq_len, input_dim)\n",
       "        \n",
       "        # Compute attention scores\n",
       "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
       "        \n",
       "        # Apply mask if provided\n",
       "        if mask is not None:\n",
       "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
       "        \n",
       "        # Compute attention weights\n",
       "        attention_weights = F.softmax(energy, dim=-1)\n",
       "        attention_weights = self.dropout(attention_weights)\n",
       "        \n",
       "        # Apply attention weights to values\n",
       "        output = torch.matmul(attention_weights, V)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Visualizing Self-Attention\n",
       "\n",
       "Let's create functions to visualize self-attention patterns:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def plot_self_attention(\n",
       "    attention_weights: torch.Tensor,\n",
       "    tokens: Optional[list] = None,\n",
       "    title: str = \"Self-Attention Weights\"\n",
       ") -> None:\n",
       "    \"\"\"Plot self-attention weights as a heatmap.\"\"\"\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    sns.heatmap(\n",
       "        attention_weights.detach().cpu().numpy(),\n",
       "        xticklabels=tokens,\n",
       "        yticklabels=tokens,\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    plt.title(title)\n",
       "    plt.xlabel('Key Position')\n",
       "    plt.ylabel('Query Position')\n",
       "    plt.show()\n",
       "\n",
       "def plot_attention_flow(\n",
       "    attention_weights: torch.Tensor,\n",
       "    tokens: list,\n",
       "    title: str = \"Attention Flow\"\n",
       ") -> None:\n",
       "    \"\"\"Plot attention flow between tokens using a directed graph.\"\"\"\n",
       "    import networkx as nx\n",
       "    \n",
       "    # Create directed graph\n",
       "    G = nx.DiGraph()\n",
       "    \n",
       "    # Add nodes\n",
       "    for i, token in enumerate(tokens):\n",
       "        G.add_node(i, label=token)\n",
       "    \n",
       "    # Add edges with weights\n",
       "    weights = attention_weights.detach().cpu().numpy()\n",
       "    for i in range(len(tokens)):\n",
       "        for j in range(len(tokens)):\n",
       "            if weights[i, j] > 0.1:  # Only show significant connections\n",
       "                G.add_edge(i, j, weight=weights[i, j])\n",
       "    \n",
       "    # Create figure\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    \n",
       "    # Draw graph\n",
       "    pos = nx.spring_layout(G)\n",
       "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=1000)\n",
       "    nx.draw_networkx_edges(G, pos, edge_color='gray', width=1, alpha=0.5)\n",
       "    nx.draw_networkx_labels(G, pos, {i: token for i, token in enumerate(tokens)})\n",
       "    \n",
       "    plt.title(title)\n",
       "    plt.axis('off')\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Example: Text Processing\n",
       "\n",
       "Let's demonstrate self-attention on a simple text example:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Create sample data\n",
       "batch_size = 1\n",
       "seq_len = 6\n",
       "hidden_dim = 8\n",
       "\n",
       "# Sample sentence: \"The cat sat on the mat\"\n",
       "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
       "\n",
       "# Create random embeddings for demonstration\n",
       "x = torch.randn(batch_size, seq_len, hidden_dim)\n",
       "\n",
       "# Initialize self-attention\n",
       "self_attention = SelfAttention(input_dim=hidden_dim)\n",
       "\n",
       "# Compute self-attention\n",
       "output, attention_weights = self_attention(x)\n",
       "\n",
       "# Visualize attention weights\n",
       "plot_self_attention(\n",
       "    attention_weights[0],\n",
       "    tokens=tokens,\n",
       "    title='Self-Attention in \"The cat sat on the mat\"'\n",
       ")\n",
       "\n",
       "# Visualize attention flow\n",
       "plot_attention_flow(\n",
       "    attention_weights[0],\n",
       "    tokens=tokens,\n",
       "    title='Attention Flow in \"The cat sat on the mat\"'\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Real-World Example: Using Pre-trained Model\n",
       "\n",
       "Let's examine self-attention in a pre-trained model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "from transformers import BertTokenizer, BertModel\n",
       "\n",
       "# Load pre-trained model and tokenizer\n",
       "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
       "model = BertModel.from_pretrained('bert-base-uncased')\n",
       "\n",
       "# Example sentence\n",
       "text = \"The quick brown fox jumps over the lazy dog\"\n",
       "\n",
       "# Tokenize and get model output\n",
       "inputs = tokenizer(text, return_tensors='pt')\n",
       "outputs = model(**inputs, output_attentions=True)\n",
       "\n",
       "# Get attention weights from the first layer\n",
       "attention_weights = outputs.attentions[0][0]  # First batch, first layer\n",
       "\n",
       "# Get tokens\n",
       "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
       "\n",
       "# Visualize attention\n",
       "plot_self_attention(\n",
       "    attention_weights[0],  # First attention head\n",
       "    tokens=tokens,\n",
       "    title='BERT Self-Attention (First Head)'\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. Implementation of self-attention\n",
       "2. Visualization of attention patterns\n",
       "3. Application to text processing\n",
       "4. Real-world example using BERT\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Self-attention allows each position to attend to all positions\n",
       "- It helps capture long-range dependencies in sequences\n",
       "- Attention weights provide interpretability\n",
       "- Different attention heads can learn different patterns"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 