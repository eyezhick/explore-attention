{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Introduction to Attention Mechanisms\n",
       "\n",
       "This notebook introduces the fundamental concepts of attention mechanisms in deep learning. We'll explore:\n",
       "\n",
       "1. Why attention is needed\n",
       "2. The basic mathematics behind attention\n",
       "3. A simple implementation from scratch\n",
       "4. Visualizing how attention works"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Why Attention?\n",
       "\n",
       "Traditional sequence models like RNNs and LSTMs have several limitations:\n",
       "\n",
       "1. **Information Bottleneck**: All information must pass through a fixed-size hidden state\n",
       "2. **Long-term Dependencies**: Difficulty in capturing relationships between distant elements\n",
       "3. **Parallelization**: Sequential processing makes it hard to parallelize\n",
       "\n",
       "Attention mechanisms address these issues by:\n",
       "\n",
       "- Allowing direct access to any part of the input sequence\n",
       "- Computing relevance scores between elements\n",
       "- Enabling parallel processing of the entire sequence"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## The Math Behind Attention\n",
       "\n",
       "The core of attention is computing relevance scores between queries and keys, then using these scores to weight the values:\n",
       "\n",
       "1. **Query-Key-Value Triplet**:\n",
       "   - Query (Q): What we're looking for\n",
       "   - Key (K): What we're matching against\n",
       "   - Value (V): What we're retrieving\n",
       "\n",
       "2. **Attention Scores**:\n",
       "   $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
       "\n",
       "Where $d_k$ is the dimension of the key vectors."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import numpy as np\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Simple Attention Implementation\n",
       "\n",
       "Let's implement a basic attention mechanism from scratch:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class SimpleAttention(nn.Module):\n",
       "    def __init__(self, input_dim):\n",
       "        super().__init__()\n",
       "        self.input_dim = input_dim\n",
       "        self.scale = math.sqrt(input_dim)\n",
       "        \n",
       "    def forward(self, query, key, value, mask=None):\n",
       "        # Compute attention scores\n",
       "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
       "        \n",
       "        # Apply mask if provided\n",
       "        if mask is not None:\n",
       "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
       "        \n",
       "        # Compute attention weights\n",
       "        attention_weights = F.softmax(scores, dim=-1)\n",
       "        \n",
       "        # Apply attention weights to values\n",
       "        output = torch.matmul(attention_weights, value)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Visualizing Attention\n",
       "\n",
       "Let's create a simple example to visualize how attention works:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def visualize_attention(attention_weights, labels):\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    sns.heatmap(attention_weights, \n",
       "                xticklabels=labels,\n",
       "                yticklabels=labels,\n",
       "                cmap='viridis')\n",
       "    plt.title('Attention Weights')\n",
       "    plt.xlabel('Key')\n",
       "    plt.ylabel('Query')\n",
       "    plt.show()\n",
       "\n",
       "# Create a simple example\n",
       "sequence_length = 5\n",
       "embedding_dim = 4\n",
       "\n",
       "# Generate random embeddings\n",
       "query = torch.randn(1, sequence_length, embedding_dim)\n",
       "key = torch.randn(1, sequence_length, embedding_dim)\n",
       "value = torch.randn(1, sequence_length, embedding_dim)\n",
       "\n",
       "# Create attention mechanism\n",
       "attention = SimpleAttention(embedding_dim)\n",
       "\n",
       "# Compute attention\n",
       "output, attention_weights = attention(query, key, value)\n",
       "\n",
       "# Visualize attention weights\n",
       "labels = [f'Token {i+1}' for i in range(sequence_length)]\n",
       "visualize_attention(attention_weights[0].detach().numpy(), labels)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Real-World Example: Machine Translation\n",
       "\n",
       "Let's see how attention helps in machine translation by visualizing the attention weights between source and target words:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "from transformers import MarianMTModel, MarianTokenizer\n",
       "\n",
       "# Load pre-trained model and tokenizer\n",
       "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
       "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
       "model = MarianMTModel.from_pretrained(model_name)\n",
       "\n",
       "# Example translation\n",
       "text = \"The cat sat on the mat.\"\n",
       "inputs = tokenizer(text, return_tensors=\"pt\")\n",
       "\n",
       "# Get translation and attention weights\n",
       "outputs = model.generate(**inputs, output_attentions=True)\n",
       "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "\n",
       "print(f\"English: {text}\")\n",
       "print(f\"French: {translation}\")\n",
       "\n",
       "# Visualize attention weights from the last layer\n",
       "attention_weights = outputs.attentions[-1][0, 0].mean(dim=0).detach().numpy()\n",
       "source_tokens = tokenizer.tokenize(text)\n",
       "target_tokens = tokenizer.tokenize(translation)\n",
       "\n",
       "plt.figure(figsize=(12, 8))\n",
       "sns.heatmap(attention_weights, \n",
       "            xticklabels=source_tokens,\n",
       "            yticklabels=target_tokens,\n",
       "            cmap='viridis')\n",
       "plt.title('Attention Weights in Translation')\n",
       "plt.xlabel('Source Tokens')\n",
       "plt.ylabel('Target Tokens')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. The motivation behind attention mechanisms\n",
       "2. The mathematical foundation of attention\n",
       "3. A simple implementation from scratch\n",
       "4. Visualization of attention weights\n",
       "5. A real-world example in machine translation\n",
       "\n",
       "In the next notebook, we'll explore different types of attention mechanisms and their specific applications."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 