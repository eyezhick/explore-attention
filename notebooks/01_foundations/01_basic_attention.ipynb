{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Basic Attention Mechanisms\n",
       "\n",
       "This notebook introduces the fundamental concepts of attention mechanisms in deep learning. We'll explore:\n",
       "\n",
       "1. The intuition behind attention\n",
       "2. Basic attention computation\n",
       "3. Implementation of additive and multiplicative attention\n",
       "4. Visualizing attention weights"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from typing import Optional, Tuple\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Basic Attention Implementation\n",
       "\n",
       "Let's implement a basic attention mechanism that can be used for both additive and multiplicative attention:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class BasicAttention(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        query_dim: int,\n",
       "        key_dim: int,\n",
       "        value_dim: int,\n",
       "        attention_type: str = 'dot',\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        self.attention_type = attention_type\n",
       "        self.query_dim = query_dim\n",
       "        self.key_dim = key_dim\n",
       "        self.value_dim = value_dim\n",
       "        \n",
       "        # For additive attention\n",
       "        if attention_type == 'additive':\n",
       "            self.attention = nn.Sequential(\n",
       "                nn.Linear(query_dim + key_dim, query_dim),\n",
       "                nn.Tanh(),\n",
       "                nn.Linear(query_dim, 1)\n",
       "            )\n",
       "        \n",
       "        # For multiplicative attention\n",
       "        elif attention_type == 'dot':\n",
       "            self.scale = torch.sqrt(torch.FloatTensor([query_dim]))\n",
       "        \n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        query: torch.Tensor,\n",
       "        key: torch.Tensor,\n",
       "        value: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        \"\"\"\n",
       "        Args:\n",
       "            query: Query tensor of shape (batch_size, num_queries, query_dim)\n",
       "            key: Key tensor of shape (batch_size, num_keys, key_dim)\n",
       "            value: Value tensor of shape (batch_size, num_keys, value_dim)\n",
       "            mask: Optional mask tensor of shape (batch_size, num_queries, num_keys)\n",
       "            \n",
       "        Returns:\n",
       "            Tuple of (output, attention_weights)\n",
       "        \"\"\"\n",
       "        batch_size = query.shape[0]\n",
       "        \n",
       "        if self.attention_type == 'additive':\n",
       "            # Additive attention\n",
       "            query = query.unsqueeze(2)  # (batch_size, num_queries, 1, query_dim)\n",
       "            key = key.unsqueeze(1)      # (batch_size, 1, num_keys, key_dim)\n",
       "            \n",
       "            # Compute attention scores\n",
       "            energy = self.attention(torch.cat([query, key], dim=-1))\n",
       "            energy = energy.squeeze(-1)  # (batch_size, num_queries, num_keys)\n",
       "            \n",
       "        else:\n",
       "            # Multiplicative (dot-product) attention\n",
       "            energy = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
       "        \n",
       "        # Apply mask if provided\n",
       "        if mask is not None:\n",
       "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
       "        \n",
       "        # Compute attention weights\n",
       "        attention_weights = F.softmax(energy, dim=-1)\n",
       "        attention_weights = self.dropout(attention_weights)\n",
       "        \n",
       "        # Apply attention weights to values\n",
       "        output = torch.matmul(attention_weights, value)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Visualizing Attention\n",
       "\n",
       "Let's create a function to visualize attention weights:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def plot_attention_weights(\n",
       "    attention_weights: torch.Tensor,\n",
       "    x_labels: Optional[list] = None,\n",
       "    y_labels: Optional[list] = None,\n",
       "    title: str = \"Attention Weights\"\n",
       ") -> None:\n",
       "    \"\"\"Plot attention weights as a heatmap.\"\"\"\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    sns.heatmap(\n",
       "        attention_weights.detach().cpu().numpy(),\n",
       "        xticklabels=x_labels,\n",
       "        yticklabels=y_labels,\n",
       "        cmap='viridis'\n",
       "    )\n",
       "    plt.title(title)\n",
       "    plt.xlabel('Key')\n",
       "    plt.ylabel('Query')\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Example: Machine Translation\n",
       "\n",
       "Let's demonstrate attention in a simple machine translation scenario:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Create sample data\n",
       "batch_size = 1\n",
       "seq_len = 5\n",
       "hidden_dim = 8\n",
       "\n",
       "# Source sequence (e.g., English)\n",
       "source = torch.randn(batch_size, seq_len, hidden_dim)\n",
       "\n",
       "# Target sequence (e.g., French)\n",
       "target = torch.randn(batch_size, seq_len, hidden_dim)\n",
       "\n",
       "# Initialize attention mechanism\n",
       "attention = BasicAttention(\n",
       "    query_dim=hidden_dim,\n",
       "    key_dim=hidden_dim,\n",
       "    value_dim=hidden_dim,\n",
       "    attention_type='dot'\n",
       ")\n",
       "\n",
       "# Compute attention\n",
       "output, attention_weights = attention(target, source, source)\n",
       "\n",
       "# Visualize attention weights\n",
       "plot_attention_weights(\n",
       "    attention_weights[0],\n",
       "    x_labels=[f'Source {i+1}' for i in range(seq_len)],\n",
       "    y_labels=[f'Target {i+1}' for i in range(seq_len)],\n",
       "    title='Attention Weights in Machine Translation'\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Comparing Additive and Multiplicative Attention\n",
       "\n",
       "Let's compare the two types of attention:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize both types of attention\n",
       "additive_attention = BasicAttention(\n",
       "    query_dim=hidden_dim,\n",
       "    key_dim=hidden_dim,\n",
       "    value_dim=hidden_dim,\n",
       "    attention_type='additive'\n",
       ")\n",
       "\n",
       "multiplicative_attention = BasicAttention(\n",
       "    query_dim=hidden_dim,\n",
       "    key_dim=hidden_dim,\n",
       "    value_dim=hidden_dim,\n",
       "    attention_type='dot'\n",
       ")\n",
       "\n",
       "# Compute attention with both mechanisms\n",
       "_, additive_weights = additive_attention(target, source, source)\n",
       "_, multiplicative_weights = multiplicative_attention(target, source, source)\n",
       "\n",
       "# Visualize both\n",
       "plt.figure(figsize=(15, 6))\n",
       "\n",
       "plt.subplot(1, 2, 1)\n",
       "sns.heatmap(additive_weights[0].detach().cpu().numpy(), cmap='viridis')\n",
       "plt.title('Additive Attention')\n",
       "plt.xlabel('Source')\n",
       "plt.ylabel('Target')\n",
       "\n",
       "plt.subplot(1, 2, 2)\n",
       "sns.heatmap(multiplicative_weights[0].detach().cpu().numpy(), cmap='viridis')\n",
       "plt.title('Multiplicative Attention')\n",
       "plt.xlabel('Source')\n",
       "plt.ylabel('Target')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Conclusion\n",
       "\n",
       "In this notebook, we've explored:\n",
       "\n",
       "1. Basic attention mechanisms (additive and multiplicative)\n",
       "2. Implementation of attention in PyTorch\n",
       "3. Visualization of attention weights\n",
       "4. Comparison of different attention types\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- Additive attention uses a feed-forward network to compute attention scores\n",
       "- Multiplicative attention uses dot products, making it more efficient\n",
       "- Both types can be effective, with multiplicative attention being more commonly used in practice\n",
       "- Attention weights provide interpretability into model decisions"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }