{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Building a Toy Transformer from Scratch\n",
       "\n",
       "This notebook demonstrates how to build a simplified Transformer model from scratch. We'll cover:\n",
       "\n",
       "1. Core components of the Transformer\n",
       "2. Implementation of each component\n",
       "3. Training on a simple task\n",
       "4. Visualizing the model's behavior"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Transformer Architecture Overview\n",
       "\n",
       "The Transformer consists of several key components:\n",
       "\n",
       "1. **Input Embedding**: Convert input tokens to vectors\n",
       "2. **Positional Encoding**: Add position information to embeddings\n",
       "3. **Multi-Head Attention**: Process relationships between tokens\n",
       "4. **Feed-Forward Network**: Transform token representations\n",
       "5. **Layer Normalization**: Stabilize training\n",
       "6. **Residual Connections**: Help with gradient flow"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import math\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from typing import Optional, Tuple\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Positional Encoding\n",
       "\n",
       "First, let's implement the positional encoding layer:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class PositionalEncoding(nn.Module):\n",
       "    def __init__(self, d_model: int, max_seq_length: int = 5000):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Create positional encoding matrix\n",
       "        pe = torch.zeros(max_seq_length, d_model)\n",
       "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
       "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
       "        \n",
       "        # Apply sine to even indices and cosine to odd indices\n",
       "        pe[:, 0::2] = torch.sin(position * div_term)\n",
       "        pe[:, 1::2] = torch.cos(position * div_term)\n",
       "        \n",
       "        # Add batch dimension and register as buffer\n",
       "        pe = pe.unsqueeze(0)\n",
       "        self.register_buffer('pe', pe)\n",
       "        \n",
       "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
       "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
       "        return x + self.pe[:, :x.size(1)]"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Multi-Head Attention\n",
       "\n",
       "Next, let's implement the multi-head attention mechanism:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class MultiHeadAttention(nn.Module):\n",
       "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
       "        super().__init__()\n",
       "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
       "        \n",
       "        self.d_model = d_model\n",
       "        self.num_heads = num_heads\n",
       "        self.head_dim = d_model // num_heads\n",
       "        \n",
       "        # Linear projections for Q, K, V\n",
       "        self.q_proj = nn.Linear(d_model, d_model)\n",
       "        self.k_proj = nn.Linear(d_model, d_model)\n",
       "        self.v_proj = nn.Linear(d_model, d_model)\n",
       "        \n",
       "        # Output projection\n",
       "        self.out_proj = nn.Linear(d_model, d_model)\n",
       "        \n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        self.scale = math.sqrt(self.head_dim)\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        query: torch.Tensor,\n",
       "        key: torch.Tensor,\n",
       "        value: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        batch_size = query.size(0)\n",
       "        \n",
       "        # Project Q, K, V\n",
       "        q = self.q_proj(query)\n",
       "        k = self.k_proj(key)\n",
       "        v = self.v_proj(value)\n",
       "        \n",
       "        # Reshape for multi-head attention\n",
       "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
       "        \n",
       "        # Compute attention scores\n",
       "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
       "        \n",
       "        # Apply mask if provided\n",
       "        if mask is not None:\n",
       "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
       "        \n",
       "        # Compute attention weights\n",
       "        attention_weights = F.softmax(scores, dim=-1)\n",
       "        attention_weights = self.dropout(attention_weights)\n",
       "        \n",
       "        # Apply attention weights to values\n",
       "        context = torch.matmul(attention_weights, v)\n",
       "        \n",
       "        # Reshape back\n",
       "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
       "        \n",
       "        # Project output\n",
       "        output = self.out_proj(context)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Feed-Forward Network\n",
       "\n",
       "Now, let's implement the feed-forward network:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class FeedForward(nn.Module):\n",
       "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
       "        super().__init__()\n",
       "        self.linear1 = nn.Linear(d_model, d_ff)\n",
       "        self.linear2 = nn.Linear(d_ff, d_model)\n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        \n",
       "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
       "        x = F.relu(self.linear1(x))\n",
       "        x = self.dropout(x)\n",
       "        x = self.linear2(x)\n",
       "        return x"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Transformer Encoder Layer\n",
       "\n",
       "Let's combine these components into an encoder layer:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class TransformerEncoderLayer(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        d_model: int,\n",
       "        num_heads: int,\n",
       "        d_ff: int,\n",
       "        dropout: float = 0.1\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Multi-head attention\n",
       "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
       "        \n",
       "        # Feed-forward network\n",
       "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
       "        \n",
       "        # Layer normalization\n",
       "        self.norm1 = nn.LayerNorm(d_model)\n",
       "        self.norm2 = nn.LayerNorm(d_model)\n",
       "        \n",
       "        # Dropout\n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        x: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "        # Self-attention with residual connection and layer normalization\n",
       "        attn_output, attention_weights = self.self_attn(x, x, x, mask)\n",
       "        x = self.norm1(x + self.dropout(attn_output))\n",
       "        \n",
       "        # Feed-forward with residual connection and layer normalization\n",
       "        ff_output = self.feed_forward(x)\n",
       "        x = self.norm2(x + self.dropout(ff_output))\n",
       "        \n",
       "        return x, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Complete Transformer Model\n",
       "\n",
       "Now, let's build the complete Transformer model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "class Transformer(nn.Module):\n",
       "    def __init__(\n",
       "        self,\n",
       "        vocab_size: int,\n",
       "        d_model: int = 512,\n",
       "        num_heads: int = 8,\n",
       "        num_layers: int = 6,\n",
       "        d_ff: int = 2048,\n",
       "        dropout: float = 0.1,\n",
       "        max_seq_length: int = 5000\n",
       "    ):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Token embedding\n",
       "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
       "        \n",
       "        # Positional encoding\n",
       "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
       "        \n",
       "        # Encoder layers\n",
       "        self.encoder_layers = nn.ModuleList([\n",
       "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
       "            for _ in range(num_layers)\n",
       "        ])\n",
       "        \n",
       "        # Output projection\n",
       "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
       "        \n",
       "        self.dropout = nn.Dropout(dropout)\n",
       "        \n",
       "    def forward(\n",
       "        self,\n",
       "        x: torch.Tensor,\n",
       "        mask: Optional[torch.Tensor] = None\n",
       "    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
       "        # Token embedding and positional encoding\n",
       "        x = self.embedding(x)\n",
       "        x = self.pos_encoding(x)\n",
       "        x = self.dropout(x)\n",
       "        \n",
       "        # Store attention weights\n",
       "        attention_weights = []\n",
       "        \n",
       "        # Process through encoder layers\n",
       "        for layer in self.encoder_layers:\n",
       "            x, attn_weights = layer(x, mask)\n",
       "            attention_weights.append(attn_weights)\n",
       "        \n",
       "        # Project to vocabulary\n",
       "        output = self.output_proj(x)\n",
       "        \n",
       "        return output, attention_weights"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Training on a Simple Task\n",
       "\n",
       "Let's train our Transformer on a simple sequence prediction task:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def generate_sequence_data(num_samples: int, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
       "    \"\"\"Generate synthetic sequence data for training.\"\"\"\n",
       "    # Generate random sequences\n",
       "    x = torch.randint(1, 10, (num_samples, seq_length))\n",
       "    \n",
       "    # Target is the sequence shifted by one position\n",
       "    y = torch.roll(x, -1, dims=1)\n",
       "    y[:, -1] = 0  # Padding token\n",
       "    \n",
       "    return x, y\n",
       "\n",
       "def train_transformer(\n",
       "    model: Transformer,\n",
       "    num_epochs: int,\n",
       "    batch_size: int,\n",
       "    seq_length: int,\n",
       "    learning_rate: float = 0.001\n",
       ") -> List[float]:\n",
       "    \"\"\"Train the Transformer model.\"\"\"\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
       "    criterion = nn.CrossEntropyLoss()\n",
       "    \n",
       "    losses = []\n",
       "    \n",
       "    for epoch in range(num_epochs):\n",
       "        # Generate training data\n",
       "        x, y = generate_sequence_data(batch_size, seq_length)\n",
       "        \n",
       "        # Forward pass\n",
       "        output, _ = model(x)\n",
       "        \n",
       "        # Compute loss\n",
       "        loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
       "        \n",
       "        # Backward pass\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "        \n",
       "        losses.append(loss.item())\n",
       "        \n",
       "        if (epoch + 1) % 10 == 0:\n",
       "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
       "    \n",
       "    return losses\n",
       "\n",
       "# Create and train model\n",
       "vocab_size = 10  # Small vocabulary for demonstration\n",
       "d_model = 64    # Small model for faster training\n",
       "num_heads = 4\n",
       "num_layers = 2\n",
       "d_ff = 256\n",
       "\n",
       "model = Transformer(\n",
       "    vocab_size=vocab_size,\n",
       "    d_model=d_model,\n",
       "    num_heads=num_heads,\n",
       "    num_layers=num_layers,\n",
       "    d_ff=d_ff\n",
       ")\n",
       "\n",
       "losses = train_transformer(\n",
       "    model=model,\n",
       "    num_epochs=100,\n",
       "    batch_size=32,\n",
       "    seq_length=10\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Visualizing Training Progress\n",
       "\n",
       "Let's plot the training loss:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "plt.figure(figsize=(10, 6))\n",
       "plt.plot(losses)\n",
       "plt.title('Training Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.grid(True)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Visualizing Attention Patterns\n",
       "\n",
       "Let's examine the attention patterns learned by our model:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def visualize_attention_patterns(\n",
       "    model: Transformer,\n",
       "    input_sequence: torch.Tensor,\n",
       "    layer_idx: int = -1\n",
       ") -> None:\n",
       "    \"\"\"Visualize attention patterns for a given input sequence.\"\"\"\n",
       "    # Get model output and attention weights\n",
       "    with torch.no_grad():\n",
       "        _, attention_weights = model(input_sequence.unsqueeze(0))\n",
       "    \n",
       "    # Get attention weights for the specified layer\n",
       "    layer_attention = attention_weights[layer_idx][0, 0]  # First batch, first head\n",
       "    \n",
       "    # Plot attention heatmap\n",
       "    plt.figure(figsize=(8, 8))\n",
       "    sns.heatmap(\n",
       "        layer_attention,\n",
       "        cmap='viridis',\n",
       "        xticklabels=input_sequence.tolist(),\n",
       "        yticklabels=input_sequence.tolist()\n",
       "    )\n",
       "    plt.title(f'Attention Patterns (Layer {layer_idx})')\n",
       "    plt.xlabel('Key Position')\n",
       "    plt.ylabel('Query Position')\n",
       "    plt.show()\n",
       "\n",
       "# Generate a test sequence\n",
       "test_sequence = torch.randint(1, 10, (10,))\n",
       "print(f\"Test sequence: {test_sequence.tolist()}\")\n",
       "\n",
       "# Visualize attention patterns for different layers\n",
       "for layer_idx in range(num_layers):\n",
       "    visualize_attention_patterns(model, test_sequence, layer_idx)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "In this notebook, we've built a toy Transformer model from scratch and explored its components:\n",
       "\n",
       "1. Positional encoding\n",
       "2. Multi-head attention\n",
       "3. Feed-forward networks\n",
       "4. Layer normalization\n",
       "5. Residual connections\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "- The Transformer architecture is modular and elegant\n",
       "- Each component serves a specific purpose\n",
       "- Attention patterns provide insights into model behavior\n",
       "\n",
       "This implementation is simplified for educational purposes. Real-world Transformers (like BERT, GPT) use more sophisticated components and training techniques."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 